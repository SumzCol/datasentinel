[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[project]
name = "dataguard"
description = "Data Guard is a powerful tool to monitor data pipelines and ensure data quality."
authors = [
    { name = "Sumz SAS" },
]
requires-python = ">=3.10"
license = {text = "Apache Software License (Apache 2.0)"}
keywords = [
    "data quality",
    "data engineering",
    "monitoring",
    "data validation",
    "data pipelines",
    "pipelines",
    "audit logging",
]
classifiers = [
    "Development Status :: 4 - Alpha",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Programming Language :: Python :: 3.13",
]
dependencies = [
  "python-ulid~=2.7.0",
  "pydantic~=2.10",
]

dynamic = ["version", "readme"]

[project.urls]
"Homepage" = "https://github.com/SumzCol/dataguard"
"Bug Tracker" = "https://github.com/SumzCol/dataguard"

[tool.setuptools.dynamic]
version = {attr = "dataguard.__version__"}
readme = {file = "README.md", content-type = "text/markdown"}

[project.optional-dependencies]
# Base dependencies
pyspark-base = [
  "pyspark>=3.4.0"
]
deltatable-base = [
  "delta-spark>=3.1.0, <4.0"
]
pandas-base = [
  "pandas>=1.3, <3.0"
]

# Checks dependencies
pandas-cualleecheck = [
  "cuallee[pandas]~=0.15.2"
]
pyspark-cualleecheck = [
  "cuallee[pyspark]~=0.15.2",
]
cualleecheck = [
  "dataguard[pyspark-cualleecheck,pandas-cualleecheck]"
]
pandas-rowlevelresultcheck = [
  "dataguard[pandas-base]",
]
pyspark-rowlevelresultcheck = [
  "dataguard[pyspark-base]",
]
rowlevelresultcheck = [
  "dataguard[pyspark-rowlevelresultcheck,pandas-rowlevelresultcheck]"
]
pandas-checks = [
  "dataguard[pandas-cualleecheck,pandas-rowlevelresultcheck]",
]
pyspark-checks = [
  "dataguard[pyspark-cualleecheck,pyspark-rowlevelresultcheck]",
]
all-checks = [
  "dataguard[pandas-checks,pyspark-checks]"
]

# Result stores dependencies
spark-deltatableresultstore = [
  "dataguard[pyspark-base,deltatable-base]",
]
all-resultstores = [
  "dataguard[spark-deltatableresultstore]"
]

# Audit stores dependencies
database-databaseauditstore = [
  "SQLAlchemy>=1.4, <3.0",
]
spark-deltatableauditstore = [
  "dataguard[pyspark-base,deltatable-base]",
]
all-auditstores = [
  "dataguard[database-databaseauditstore,spark-deltatableauditstore]"
]

# Notifiers dependencies
slack-slacknotifier = [
  "slack-sdk~=3.34.0"
]
all-notifiers = [
  "dataguard[slack-slacknotifier]"
]

# Testing dependencies
test = [
  "pytest>=7.2,<9.0",
  "pytest-cov>=3,<7",
  "pendulum >= 2.1.2",
  "coverage[toml]",
  "dataguard[all-checks,all-resultstores,all-auditstores,all-notifiers]",
]

# Scripts dependencies
scripts = [
    "click==8.1.0"
]

# Lint dependencies
lint = [
  "ruff==0.9.7",
  "pre-commit>=2.9.2, <5.0",
  "mypy~=1.15.0",
]

all = [
  "dataguard[test,scripts,lint]"
]

[tool.ruff]
line-length = 98
show-fixes = true
lint.select = [
    "F",    # Pyflakes
    "W",    # pycodestyle
    "E",    # pycodestyle
    "I",    # isort
    "UP",   # pyupgrade
    "PL",   # Pylint
    "T201", # Print Statement
    "S",    # flake8-bandit
    "TCH",  # flake8-type-checking
    "RUF",  # Ruff-specific rules
]

[tool.ruff.lint.per-file-ignores]
"tests/*" = ["UP007", "S101"]

[tool.ruff.lint.pylint]
max-args = 7  # Set the maximum number of arguments for a function

[tool.ruff.lint.isort]
known-first-party = ["dataguard"]

[tool.mypy]
allow_any_generics = true
no_warn_unused_ignores = true
ignore_missing_imports = true
disable_error_code = ['misc']
exclude = ['^docs/']

[tool.pytest.ini_options]
addopts="""
--cov-context test  \
--cov-config pyproject.toml \
--cov-report xml:coverage.xml \
--cov-report term-missing \
--cov dataguard \
--cov tests \
--no-cov-on-fail \
-ra \
-W ignore"""
markers = [
    "unit: mark a test as a unit test",
    "functional: mark a test as a functional test",
    "slow: mark a test as a slow test",
]
testpaths = [
  "tests"
]

[tool.coverage.report]
fail_under = 85
show_missing = true
omit = [
    "tests/*",
]
exclude_also = ["raise NotImplementedError", "if TYPE_CHECKING:"]

[tool.coverage.run]
concurrency = ["multiprocessing", "thread"]
parallel = true
sigterm = true
